### ğŸŒŸğŸŒŸğŸŒŸ Find interesting work or want your work to be on-board? Raise an Issue or Pull Requests! :)
## Contributors
We thank the following contributors for their valuable contributions!
[zwenyu](https://github.com/zwenyu), 
[Yuan-ManX](https://github.com/Yuan-ManX), 
[chaoweihuang](https://github.com/chaoweihuang), 
[Liu-Tianchi](https://github.com/Liu-Tianchi), 
[Sakshi113](https://github.com/Sakshi113), 
[hbwu-ntu](https://github.com/hbwu-ntu), 
[potsawee](https://github.com/potsawee), 
[czwxian](https://github.com/czwxian), 
[marianasignal](https://github.com/marianasignal), 
and You!
[![Star History Chart](https://api.star-history.com/svg?repos=AudioLLMs/Awesome-Audio-LLM&type=Date)](https://star-history.com/#AudioLLMs/Awesome-Audio-LLM&Date)
## Table of Contents
- [Dataset Resource](#dataset-resource)
- [Model and Methods](#model-and-methods)
- [Benchmark](#benchmark)
- [Multimodal](#multimodal)
- [Survey](#survey)
- [Study](#study)
- [Safety](#safety)
- [Chatbot](#chatbot)

<img src="model_release_timeline_vertical_listed.png" alt="Timeline Visualization" width="600">

### Abbreviations with Links
- [OSUM](https://arxiv.org/pdf/2501.13306)
- [Step-Audio](https://arxiv.org/abs/2502.11946)
- [Audio-CoT](https://arxiv.org/abs/2501.07246)
- [LUCY](https://arxiv.org/abs/2501.16327)
- [MinMo](https://arxiv.org/abs/2501.06282)
- [TalkArena](https://talkarena.org/)
- [Typhoon2-Audio](https://arxiv.org/abs/2412.13702)
- [MERaLiON-AudioLLM](https://arxiv.org/abs/2412.09818)
- [ADU-Bench](https://arxiv.org/abs/2412.05167)
- [Dynamic-SUPERB Phase-2](https://arxiv.org/pdf/2411.05361)
- [Taiwanese AudioLLM](https://arxiv.org/pdf/2411.07111)
- [WavChat-Survey](https://arxiv.org/abs/2411.13577)
- [SpeechLLM-Survey](https://arxiv.org/pdf/2410.18908v2)
- [VoiceBench](https://arxiv.org/pdf/2410.17196)
- [SPIRIT LM](https://arxiv.org/pdf/2402.05755)
- [DiVA](https://arxiv.org/pdf/2410.02678)
- [SPIRIT LM](https://arxiv.org/abs/2402.05755)
- [SpeechEmotionLlama](https://arxiv.org/pdf/2410.01162)
- [SpeechLM-Survey](https://arxiv.org/pdf/2410.03751)
- [MMAU](https://arxiv.org/pdf/2410.19168)
- [SALMon](https://arxiv.org/abs/2409.07437)
- [EMOVA](https://arxiv.org/pdf/2409.18042)
- [Moshi](https://arxiv.org/pdf/2410.00037)
- [LLaMA-Omni](https://arxiv.org/pdf/2409.06666v1)
- [Ultravox](https://github.com/fixie-ai/ultravox)
- [MoWE-Audio](https://arxiv.org/pdf/2409.06635)
- [AudioBERT](https://arxiv.org/pdf/2409.08199)
- [DeSTA2](https://arxiv.org/pdf/2409.20007)
- [ASRCompare](https://arxiv.org/pdf/2409.00800v1)
- [MooER](https://arxiv.org/pdf/2408.05101)
- [MuChoMusic](https://arxiv.org/abs/2408.01337)
- [Typhoon-Audio](https://arxiv.org/abs/2409.10999)
- [Mini-Omni](https://arxiv.org/pdf/2408.16725)
- [FunAudioLLM](https://arxiv.org/pdf/2407.04051v3)
- [Qwen2-Audio](https://arxiv.org/pdf/2407.10759)
- [GAMA](https://arxiv.org/abs/2406.11768)
- [LLaST](https://arxiv.org/pdf/2407.15415)
- [Decoder-only LLMs for STT](https://arxiv.org/pdf/2407.03169)
- [AudioEntailment](https://arxiv.org/pdf/2407.18062)
- [CompA](https://arxiv.org/abs/2310.08753)
- [DeSTA](https://arxiv.org/abs/2406.18871)
- [Audio Hallucination](https://arxiv.org/pdf/2406.08402)
- [CodecFake](https://arxiv.org/abs/2406.07237)
- [SD-Eval](https://arxiv.org/pdf/2406.13340)
- [Speech ReaLLM](https://arxiv.org/pdf/2406.09569)
- [AudioBench](https://arxiv.org/abs/2406.16020)
- [AIR-Bench](https://aclanthology.org/2024.acl-long.109/)
- [Audio Flamingo](https://arxiv.org/abs/2402.01831)
- [VoiceJailbreak](https://arxiv.org/pdf/2405.19103)
- [LibriSQA](https://arxiv.org/abs/2308.10390)
- [SALMONN](https://arxiv.org/pdf/2310.13289.pdf)
- [WavLLM](https://arxiv.org/pdf/2404.00656)
- [SpokenWOZ](https://arxiv.org/abs/2305.13040)
- [AudioLM-Survey](https://arxiv.org/abs/2402.13236)
- [SLAM-LLM](https://arxiv.org/pdf/2402.08846)
- [Pengi](https://arxiv.org/pdf/2305.11834.pdf)
- [Qwen-Audio](https://arxiv.org/pdf/2311.07919.pdf)
- [CoDi-2](https://arxiv.org/pdf/2311.18775)
- [UniAudio](https://arxiv.org/abs/2310.00704)
- [Dynamic-SUPERB](https://arxiv.org/abs/2309.09510)
- [LLaSM](https://arxiv.org/pdf/2308.15930.pdf)
- [Segment-level Q-Former](https://arxiv.org/pdf/2309.13963)
- [Prompting LLMs with Speech Recognition](https://arxiv.org/pdf/2307.11795)
- [Macaw-LLM](https://arxiv.org/pdf/2306.09093)
- [SpeechGPT](https://arxiv.org/pdf/2305.11000.pdf)
- [AudioGPT](https://arxiv.org/pdf/2304.12995.pdf)

## Dataset Resource

- `ã€2024-04ã€‘-ã€LibriSQAã€‘-ã€Shanghai Jiao Tong Universityã€‘-ã€Type: Dataset Resourceã€‘`
  - **LibriSQA: A Novel Dataset and Framework for Spoken Question Answering with Large Language Models**
  - **Author(s):** Zihan Zhao, Yiyang Jiang, Heyang Liu, Yanfeng Wang, Yu Wang
  - [![GitHub stars](https://img.shields.io/github/stars/ZihanZhaoSJTU/LibriSQA?style=social)](https://github.com/ZihanZhaoSJTU/LibriSQA)
  - [Paper](https://arxiv.org/abs/2308.10390)

## Model and Methods

- `ã€2025-02ã€‘-ã€OSUMã€‘-ã€ASLP@NPUã€‘-ã€Type: Modelã€‘`
  - **OSUM: Advancing Open Speech Understanding Models with Limited Resources in Academia**
  - **Author(s):** Xuelong Geng, Kun Wei, Qijie Shao, Shuiyun Liu, Zhennan Lin, Zhixian Zhao, Guojian Li, Wenjie Tian, Peikun Chen, Yangze Li, Pengcheng Guo, Mingchen Shao, Shuiyuan Wang, Yuang Cao, Chengyou Wang, Tianyi Xu, Yuhang Dai, Xinfa Zhu, Yue Li, Li Zhang, Lei Xie
  - [![GitHub stars](https://img.shields.io/github/stars/ASLP-lab/OSUM?style=social)](https://github.com/ASLP-lab/OSUM)
  - [Paper](https://arxiv.org/pdf/2501.13306) / [Hugging Face Model](https://huggingface.co/spaces/ASLP-lab/OSUM)

- `ã€2025-02ã€‘-ã€Step-Audioã€‘-ã€Step-Audio Team, StepFunã€‘-ã€Type: Modelã€‘`
  - **Step-Audio: Unified Understanding and Generation in Intelligent Speech Interaction**
  - **Author(s):** Ailin Huang, Boyong Wu, Bruce Wang, Chao Yan, Chen Hu, Chengli Feng, Fei Tian, Feiyu Shen, Jingbei Li, Mingrui Chen, Peng Liu, Ruihang Miao, Wang You, Xi Chen, Xuerui Yang, Yechang Huang, Yuxiang Zhang, Zheng Gong, Zixin Zhang, Hongyu Zhou, Jianjian Sun, Brian Li, Chengting Feng, Changyi Wan, Hanpeng Hu, Jianchang Wu, Jiangjie Zhen, Ranchen Ming, Song Yuan, Xuelin Zhang, Yu Zhou, Bingxin Li, Buyun Ma, Hongyuan Wang, Kang An, Wei Ji, Wen Li, Xuan Wen, Xiangwen Kong, Yuankai Ma, Yuanwei Liang, Yun Mou, Bahtiyar Ahmidi, Bin Wang, Bo Li, Changxin Miao, Chen Xu, Chenrun Wang, Dapeng Shi, Deshan Sun, Dingyuan Hu, Dula Sai, Enle Liu, Guanzhe Huang, Gulin Yan, Heng Wang, Haonan Jia, Haoyang Zhang, Jiahao Gong, Junjing Guo, Jiashuai Liu, Jiahong Liu, Jie Feng, Jie Wu, Jiaoren Wu, Jie Yang, Jinguo Wang, Jingyang Zhang, Junzhe Lin, Kaixiang Li, Lei Xia, Li Zhou, Liang Zhao, Longlong Gu, Mei Chen, Menglin Wu, Ming Li, Mingxiao Li, Mingliang Li, Mingyao Liang, Na Wang, Nie Hao, Qiling Wu, Qinyuan Tan, Ran Sun, Shuai Shuai, Shaoliang Pang, Shiliang Yang, Shuli Gao, Shanshan Yuan, Siqi Liu, Shihong Deng, Shilei Jiang, Sitong Liu, Tiancheng Cao, Tianyu Wang, Wenjin Deng, Wuxun Xie, Weipeng Ming, Wenqing He , Wen Sun, Xin Han, Xin Huang, Xiaomin Deng, Xiaojia Liu, Xin Wu, Xu Zhao, Yanan Wei, Yanbo Yu, Yang Cao, Yangguang Li, Yangzhen Ma, Yanming Xu, Yaoyu Wang, Yaqiang Shi, Yilei Wang, Yizhuang Zhou, Yinmin Zhong, Yang Zhang, Yaoben Wei, Yu Luo, Yuanwei Lu, Yuhe Yin, Yuchu Luo, Yuanhao Ding, Yuting Yan, Yaqi Dai, Yuxiang Yang, Zhe Xie, Zheng Ge, Zheng Sun, Zhewei Huang, Zhichao Chang, Zhisheng Guan, Zidong Yang, Zili Zhang, Binxing Jiao, Daxin Jiang, Heung-Yeung Shum, Jiansheng Chen, Jing Li, Shuchang Zhou, Xiangyu Zhang, Xinhao Zhang, Yibo Zhu
  - [![GitHub stars](https://img.shields.io/github/stars/stepfun-ai/Step-Audio?style=social)](https://github.com/stepfun-ai/Step-Audio)
  - [Paper](https://arxiv.org/abs/2502.11946) / [Hugging Face Model](https://huggingface.co/stepfun-ai/Step-Audio-Chat)

- `ã€2025-01ã€‘-ã€Audio-CoTã€‘-ã€Nanyang Technological University, Singaporeã€‘-ã€Type: Modelã€‘`
  - **Audio-CoT: Exploring Chain-of-Thought Reasoning in Large Audio Language Model**
  - **Author(s):** Ziyang Ma, Zhuo Chen, Yuping Wang, Eng Siong Chng, Xie Chen
  - [Paper](https://arxiv.org/abs/2501.07246)

- `ã€2025-01ã€‘-ã€LUCYã€‘-ã€Tencentã€‘-ã€Type: Modelã€‘`
  - **LUCY: Linguistic Understanding and Control Yielding Early Stage of Her**
  - **Author(s):** Heting Gao, Hang Shao, Xiong Wang, Chaofan Qiu, Yunhang Shen, Siqi Cai, Yuchen Shi, Zihan Xu, Zuwei Long, Yike Zhang, Shaoqi Dong, Chaoyou Fu, Ke Li, Long Ma, Xing Sun
  - [![GitHub stars](https://img.shields.io/github/stars/VITA-MLLM/LUCY?style=social)](https://github.com/VITA-MLLM/LUCY)
  - [Paper](https://arxiv.org/abs/2501.16327)

- `ã€2024-12ã€‘-ã€Typhoon2-Audioã€‘-ã€SCB 10Xã€‘-ã€Type: Multimodal Language Modelã€‘`
  - **Typhoon2-Audio: A Thai Multimodal Language Model for Speech and Text Processing**
  - **Author(s):** Kunat Pipatanakul, Potsawee Manakul, Natapong Nitarach, Warit Sirichotedumrong, Surapon Nonesung, Teetouch Jaknamon, Parinthapat Pengpun, Pittawat Taveekitworachai, Adisai Na-Thalang, Sittipong Sripaisarnmongkol, Krisanapong Jirayoot, Kasima Tharnpipitchai
  - [![GitHub stars](https://img.shields.io/github/stars/typhoon2-audio/?style=social)](https://github.com/scb-10x/typhoon2-audio/)
  - [Paper](https://arxiv.org/abs/2412.13702) / [Hugging Face Model](https://huggingface.co/scb10x/llama3.1-typhoon2-audio-8b-instruct) / [Demo](https://audio.opentyphoon.ai/)

- `ã€2024-12ã€‘-ã€MERaLiON-AudioLLMã€‘-ã€I2R, A*STAR, Singaporeã€‘-ã€Type: Modelã€‘`
  - **MERaLiON-AudioLLM: Bridging Audio and Language with Large Language Models**
  - **Author(s):** Yingxu He, Zhuohan Liu, Shuo Sun, Bin Wang, Wenyu Zhang, Xunlong Zou, Nancy F. Chen, Ai Ti Aw
  - [Paper](https://arxiv.org/abs/2412.09818) / [Hugging Face Model](https://huggingface.co/MERaLiON/MERaLiON-AudioLLM-Whisper-SEA-LION) / [Demo](https://huggingface.co/spaces/MERaLiON/MERaLiON-AudioLLM)

- `ã€2024-11ã€‘-ã€Taiwanese AudioLLMã€‘-ã€National Taiwan Universityã€‘-ã€Type: Modelã€‘`
  - **Building a Taiwanese Mandarin Spoken Language Model: A First Attempt**
  - **Author(s):** Chih-Kai Yang, Yu-Kuan Fu, Chen-An Li, Yi-Cheng Lin, Yu-Xiang Lin, Wei-Chih Chen, Ho Lam Chung, Chun-Yi Kuan, Wei-Ping Huang, Ke-Han Lu, Tzu-Quan Lin, Hsiu-Hsuan Wang, En-Pei Hu, Chan-Jan Hsu, Liang-Hsuan Tseng, I-Hsiang Chiu, Ulin Sanga, Xuanjun Chen, Po-chun Hsu, Shu-wen Yang, Hung-yi Lee
  - [Paper](https://arxiv.org/pdf/2411.07111)

- `ã€2024-10ã€‘-ã€SPIRIT LMã€‘-ã€Metaã€‘-ã€Type: Modelã€‘`
  - **SPIRIT LM: Interleaved Spoken and Written Language Model**
  - **Author(s):** Tu Anh Nguyen, Benjamin Muller, Bokai Yu, Marta R. Costa-jussa, Maha Elbayad, Sravya Popuri, Paul-Ambroise Duquenne, Robin Algayres, Ruslan Mavlyutov, Itai Gat, Gabriel Synnaeve, Juan Pino, BenoÃ®t Sagot, Emmanuel Dupoux
  - [![GitHub stars](https://img.shields.io/github/stars/facebookresearch/spiritlm?style=social)](https://github.com/facebookresearch/spiritlm)
  - [Paper](https://arxiv.org/pdf/2402.05755) / [Demo](https://speechbot.github.io/spiritlm/)

- `ã€2024-10ã€‘-ã€DiVAã€‘-ã€Georgia Tech, Stanfordã€‘-ã€Type: Modelã€‘`
  - **Distilling an End-to-End Voice Assistant Without Instruction Training Data**
  - **Author(s):** William Held, Ella Li, Michael Ryan, Weiyan Shi, Yanzhe Zhang, Diyi Yang
  - [![GitHub stars](https://img.shields.io/github/stars/github.com/diva-audio?style=social)](https://github.com/diva-audio)
  - [Paper](https://arxiv.org/pdf/2410.02678) / [Demo](https://diva-audio.github.io/)

- `ã€2024-10ã€‘-ã€SPIRIT LMã€‘-ã€Metaã€‘-ã€Type: Modelã€‘`
  - **SPIRIT LM: Interleaved Spoken and Written Language Model**
  - **Author(s):** Tu Anh Nguyen, Benjamin Muller, Bokai Yu, Marta R. Costa-jussa, Maha Elbayad, Sravya Popuri, Christophe Ropers, Paul-Ambroise Duquenne, Robin Algayres, Ruslan Mavlyutov, Itai Gat, Mary Williamson, Gabriel Synnaeve, Juan Pino, Benoit Sagot, Emmanuel Dupoux
  - [![GitHub stars](https://img.shields.io/github/stars/facebookresearch/spiritlm?style=social)](https://github.com/facebookresearch/spiritlm)
  - [Paper](https://arxiv.org/abs/2402.05755) / [Other Link](https://speechbot.github.io/spiritlm/)

- `ã€2024-10ã€‘-ã€SpeechEmotionLlamaã€‘-ã€MIT, Metaã€‘-ã€Type: Modelã€‘`
  - **Frozen Large Language Models Can Perceive Paralinguistic Aspects of Speech**
  - **Author(s):** Wonjune Kang, Junteng Jia, Chunyang Wu, Wei Zhou, Egor Lakomkin, Yashesh Gaur, Leda Sari, Suyoun Kim, Ke Li, Jay Mahadeokar, Ozlem Kalinli
  - [Paper](https://arxiv.org/pdf/2410.01162)

- `ã€2024-09ã€‘-ã€Moshiã€‘-ã€Kyutaiã€‘-ã€Type: Modelã€‘`
  - **Moshi: a speech-text foundation model for real-time dialogue**
  - **Author(s):** Alexandre DÃ©fossez, Laurent MazarÃ©, Manu Orsini, AmÃ©lie Royer, Patrick PÃ©rez, HervÃ© JÃ©gou, Edouard Grave, Neil Zeghidour
  - [![GitHub stars](https://img.shields.io/github/stars/kyutai-labs/moshi?style=social)](https://github.com/kyutai-labs/moshi)
  - [Paper](https://arxiv.org/pdf/2410.00037)

- `ã€2024-09ã€‘-ã€LLaMA-Omniã€‘-ã€Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS)ã€‘-ã€Type: Modelã€‘`
  - **LLaMA-Omni: Seamless Speech Interaction with Large Language Models**
  - **Author(s):** Qingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma, Shaolei Zhang, Yang Feng
  - [![GitHub stars](https://img.shields.io/github/stars/ictnlp/llama-omni?style=social)](https://github.com/ictnlp/llama-omni)
  - [Paper](https://arxiv.org/pdf/2409.06666v1)

- `ã€2024-09ã€‘-ã€Ultravoxã€‘-ã€Fixie.aiã€‘-ã€Type: Modelã€‘`
  - **Ultravox: A Fast Multimodal LLM for Real-Time Voice**
  - **Author(s):** 
  - [![GitHub stars](https://img.shields.io/github/stars/fixie-ai/ultravox?style=social)](https://github.com/fixie-ai/ultravox)

- `ã€2024-09ã€‘-ã€MoWE-Audioã€‘-ã€A*STARã€‘-ã€Type: Modelã€‘`
  - **MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders**
  - **Author(s):** Wenyu Zhang, Shuo Sun, Bin Wang, Xunlong Zou, Zhuohan Liu, Yingxu He, Geyu Lin, Nancy F. Chen, Ai Ti Aw
  - [Paper](https://arxiv.org/pdf/2409.06635)

- `ã€2024-09ã€‘-ã€AudioBERTã€‘-ã€POSTECH, Inha Universityã€‘-ã€Type: Modelã€‘`
  - **AudioBERT: Audio Knowledge Augmented Language Model**
  - **Author(s):** Hyunjong Ok, Suho Yoo, Jaeho Lee
  - [![GitHub stars](https://img.shields.io/github/stars/HJ-Ok/AudioBERT?style=social)](https://github.com/HJ-Ok/AudioBERT)
  - [Paper](https://arxiv.org/pdf/2409.08199)

- `ã€2024-09ã€‘-ã€DeSTA2ã€‘-ã€National Taiwan University, NVIDIAã€‘-ã€Type: Modelã€‘`
  - **Developing Instruction-Following Speech Language Model Without Speech Instruction-Tuning Data**
  - **Author(s):** Ke-Han Lu, Zhehuai Chen, Szu-Wei Fu, Chao-Han Huck Yang, Jagadeesh Balam, Boris Ginsburg, Yu-Chiang Frank Wang, Hung-yi Lee
  - [![GitHub stars](https://img.shields.io/github/stars/kehanlu/DeSTA2?style=social)](https://github.com/kehanlu/DeSTA2)
  - [Paper](https://arxiv.org/pdf/2409.20007)

- `ã€2024-09ã€‘-ã€ASRCompareã€‘-ã€Tsinghua University, Tencent AI Labã€‘-ã€Type: Modelã€‘`
  - **Comparing Discrete and Continuous Space LLMs for Speech Recognition**
  - **Author(s):** Yaoxun Xu, Shi-Xiong Zhang, Jianwei Yu, Zhiyong Wu, Dong Yu
  - [![GitHub stars](https://img.shields.io/github/stars/xuyaoxun/ASRCompare?style=social)](https://github.com/xuyaoxun/ASRCompare)
  - [Paper](https://arxiv.org/pdf/2409.00800v1)

- `ã€2024-08ã€‘-ã€MooERã€‘-ã€Moore Threadsã€‘-ã€Type: Modelã€‘`
  - **MooER: LLM-based Speech Recognition and Translation Models from Moore Threads**
  - **Author(s):** Zhenlin Liang, Junhao Xu, Yi Liu, Yichao Hu, Jian Li, Yajun Zheng, Meng Cai, Hua Wang
  - [![GitHub stars](https://img.shields.io/github/stars/MooreThreads/MooER?style=social)](https://github.com/MooreThreads/MooER)
  - [Paper](https://arxiv.org/pdf/2408.05101)

- `ã€2024-08ã€‘-ã€Typhoon-Audioã€‘-ã€SCB 10Xã€‘-ã€Type: Multimodal Language Modelã€‘`
  - **Typhoon-Audio: Enhancing Low-Resource Language and Instruction Following Capabilities of Audio Language Models**
  - **Author(s):** Potsawee Manakul, Guangzhi Sun, Warit Sirichotedumrong, Kasima Tharnpipitchai, Kunat Pipatanakul
  - [Paper](https://arxiv.org/abs/2409.10999) / [Hugging Face Model](https://huggingface.co/scb10x/llama-3-typhoon-v1.5-8b-audio-preview)

- `ã€2024-08ã€‘-ã€Mini-Omniã€‘-ã€Tsinghua Universityã€‘-ã€Type: Modelã€‘`
  - **Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming**
  - **Author(s):** Zhifei Xie, Changqiao Wu
  - [![GitHub stars](https://img.shields.io/github/stars/gpt-omni/mini-omni?style=social)](https://github.com/gpt-omni/mini-omni)
  - [Paper](https://arxiv.org/pdf/2408.16725)

- `ã€2024-07ã€‘-ã€FunAudioLLMã€‘-ã€Alibabaã€‘-ã€Type: Modelã€‘`
  - **FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs**
  - **Author(s):** Authors not specified in the provided information
  - [![GitHub stars](https://img.shields.io/github/stars/github.com/FunAudioLLM?style=social)](https://github.com/FunAudioLLM)
  - [Paper](https://arxiv.org/pdf/2407.04051v3) / [Demo](https://fun-audio-llm.github.io/)

- `ã€2024-07ã€‘-ã€Qwen2-Audioã€‘-ã€Alibaba Groupã€‘-ã€Type: Modelã€‘`
  - **Qwen2-Audio Technical Report**
  - **Author(s):** Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, Chang Zhou, Jingren Zhou
  - [![GitHub stars](https://img.shields.io/github/stars/QwenLM/Qwen2-Audio?style=social)](https://github.com/QwenLM/Qwen2-Audio)
  - [Paper](https://arxiv.org/pdf/2407.10759)

- `ã€2024-07ã€‘-ã€GAMAã€‘-ã€University of Maryland, College Parkã€‘-ã€Type: Modelã€‘`
  - **GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities**
  - **Author(s):** Sreyan Ghosh, Sonal Kumar, Ashish Seth, Chandra Kiran Reddy Evuru, Utkarsh Tyagi, S Sakshi, Oriol Nieto, Ramani Duraiswami, Dinesh Manocha
  - [![GitHub stars](https://img.shields.io/github/stars/Sreyan88/GAMA?style=social)](https://github.com/Sreyan88/GAMA)
  - [Paper](https://arxiv.org/abs/2406.11768) / [Demo](https://sreyan88.github.io/gamaaudio/)

- `ã€2024-07ã€‘-ã€LLaSTã€‘-ã€The Chinese University of Hong Kong, Shenzhen; Shanghai AI Laboratory; Nara Institute of Science and Technology, Japanã€‘-ã€Type: Modelã€‘`
  - **LLaST: Improved End-to-end Speech Translation System Leveraged by Large Language Models**
  - **Author(s):** Xi Chen, Songyang Zhang, Qibing Bai, Kai Chen, Satoshi Nakamura
  - [![GitHub stars](https://img.shields.io/github/stars/openaudiolab/LLaST?style=social)](https://github.com/openaudiolab/LLaST)
  - [Paper](https://arxiv.org/pdf/2407.15415)

- `ã€2024-07ã€‘-ã€Decoder-only LLMs for STTã€‘-ã€NTU-Taiwan, Metaã€‘-ã€Type: Researchã€‘`
  - **Investigating Decoder-only Large Language Models for Speech-to-text Translation**
  - **Author(s):** Authors not specified in the provided information
  - [Paper](https://arxiv.org/pdf/2407.03169)

- `ã€2024-07ã€‘-ã€CompAã€‘-ã€University of Maryland, College Park; Adobe, USA; NVIDIA, Bangalore, Indiaã€‘-ã€Type: Modelã€‘`
  - **CompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models**
  - **Author(s):** Sreyan Ghosh, Ashish Seth, Sonal Kumar, Utkarsh Tyagi, Chandra Kiran Evuru, S. Ramaneswaran, S. Sakshi, Oriol Nieto, Ramani Duraiswami, Dinesh Manocha
  - [![GitHub stars](https://img.shields.io/github/stars/Sreyan88/CompA?style=social)](https://github.com/Sreyan88/CompA)
  - [Paper](https://arxiv.org/abs/2310.08753) / [Demo](https://sreyan88.github.io/compa_iclr/)

- `ã€2024-06ã€‘-ã€DeSTAã€‘-ã€NTU-Taiwan, Nvidiaã€‘-ã€Type: Modelã€‘`
  - **DeSTA: Enhancing Speech Language Models through Descriptive Speech-Text Alignment**
  - **Author(s):** Authors not specified in the provided information
  - [![GitHub stars](https://img.shields.io/github/stars/multimodal/DeSTA?style=social)](https://github.com/kehanlu/Nemo/tree/desta/examples/multimodal/DeSTA)
  - [Paper](https://arxiv.org/abs/2406.18871)

- `ã€2024-06ã€‘-ã€Speech ReaLLMã€‘-ã€Metaã€‘-ã€Type: Modelã€‘`
  - **Speech ReaLLM â€“ Real-time Streaming Speech Recognition with Multimodal LLMs by Teaching the Flow of Time**
  - **Author(s):** Authors not specified in the provided information
  - [Paper](https://arxiv.org/pdf/2406.09569)

- `ã€2024-05ã€‘-ã€Audio Flamingoã€‘-ã€Nvidiaã€‘-ã€Type: Modelã€‘`
  - **Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities**
  - **Author(s):** Authors not specified in the provided information
  - [![GitHub stars](https://img.shields.io/github/stars/NVIDIA/audio-flamingo?style=social)](https://github.com/NVIDIA/audio-flamingo)
  - [Paper](https://arxiv.org/abs/2402.01831)

- `ã€2024-04ã€‘-ã€SALMONNã€‘-ã€Tsinghuaã€‘-ã€Type: Modelã€‘`
  - **SALMONN: Towards Generic Hearing Abilities for Large Language Models**
  - **Author(s):** Authors not specified in the provided information
  - [![GitHub stars](https://img.shields.io/github/stars/bytedance/SALMONN?style=social)](https://github.com/bytedance/SALMONN)
  - [Paper](https://arxiv.org/pdf/2310.13289.pdf) / [Demo](https://huggingface.co/spaces/tsinghua-ee/SALMONN-7B-gradio)

- `ã€2024-03ã€‘-ã€WavLLMã€‘-ã€CUHKã€‘-ã€Type: Modelã€‘`
  - **WavLLM: Towards Robust and Adaptive Speech Large Language Model**
  - **Author(s):** Authors not specified in the provided information
  - [![GitHub stars](https://img.shields.io/github/stars/main/WavLLM?style=social)](https://github.com/microsoft/SpeechT5/tree/main/WavLLM)
  - [Paper](https://arxiv.org/pdf/2404.00656)

- `ã€2024-02ã€‘-ã€SLAM-LLMã€‘-ã€Shanghai Jiao Tong University (SJTU)ã€‘-ã€Type: Modelã€‘`
  - **An Embarrassingly Simple Approach for LLM with Strong ASR Capacity**
  - **Author(s):** Authors not specified in the provided information
  - [![GitHub stars](https://img.shields.io/github/stars/X-LANCE/SLAM-LLM?style=social)](https://github.com/X-LANCE/SLAM-LLM)
  - [Paper](https://arxiv.org/pdf/2402.08846)

- `ã€2024-01ã€‘-ã€Pengiã€‘-ã€Microsoftã€‘-ã€Type: Modelã€‘`
  - **Pengi: An Audio Language Model for Audio Tasks**
  - **Author(s):** Authors not specified in the provided information
  - [![GitHub stars](https://img.shields.io/github/stars/microsoft/Pengi?style=social)](https://github.com/microsoft/Pengi)
  - [Paper](https://arxiv.org/pdf/2305.11834.pdf)

- `ã€2023-12ã€‘-ã€Qwen-Audioã€‘-ã€Alibabaã€‘-ã€Type: Modelã€‘`
  - **Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models**
  - **Author(s):** Authors not specified in the provided information
  - [![GitHub stars](https://img.shields.io/github/stars/QwenLM/Qwen-Audio?style=social)](https://github.com/QwenLM/Qwen-Audio)
  - [Paper](https://arxiv.org/pdf/2311.07919.pdf) / [Demo](https://qwen-audio.github.io/Qwen-Audio/)

- `ã€2023-10ã€‘-ã€UniAudioã€‘-ã€Chinese University of Hong Kong (CUHK)ã€‘-ã€Type: Modelã€‘`
  - **An Audio Foundation Model Toward Universal Audio Generation**
  - **Author(s):** Authors not specified in the provided information
  - [![GitHub stars](https://img.shields.io/github/stars/yangdongchao/UniAudio?style=social)](https://github.com/yangdongchao/UniAudio)
  - [Paper](https://arxiv.org/abs/2310.00704) / [Demo](https://dongchaoyang.top/UniAudio_demo/)

- `ã€2023-09ã€‘-ã€LLaSMã€‘-ã€LinkSoul.AIã€‘-ã€Type: Modelã€‘`
  - **LLaSM: Large Language and Speech Model**
  - **Author(s):** Authors not specified in the provided information
  - [![GitHub stars](https://img.shields.io/github/stars/LinkSoul-AI/LLaSM?style=social)](https://github.com/LinkSoul-AI/LLaSM)
  - [Paper](https://arxiv.org/pdf/2308.15930.pdf)

- `ã€2023-09ã€‘-ã€Segment-level Q-Formerã€‘-ã€Tsinghua University, ByteDanceã€‘-ã€Type: Modelã€‘`
  - **Connecting Speech Encoder and Large Language Model for ASR**
  - **Author(s):** Wenyi Yu, Changli Tang, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, Chao Zhang
  - [Paper](https://arxiv.org/pdf/2309.13963)

- `ã€2023-07ã€‘-ã€Prompting LLMs with Speech Recognitionã€‘-ã€Metaã€‘-ã€Type: Modelã€‘`
  - **Prompting Large Language Models with Speech Recognition Abilities**
  - **Author(s):** Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Junteng Jia, Yuan Shangguan, Ke Li, Jinxi Guo, Wenhan Xiong, Jay Mahadeokar, Ozlem Kalinli, Christian Fuegen, Mike Seltzer
  - [Paper](https://arxiv.org/pdf/2307.11795)

- `ã€2023-05ã€‘-ã€SpeechGPTã€‘-ã€Fudan Universityã€‘-ã€Type: Modelã€‘`
  - **SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities**
  - **Author(s):** Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, Xipeng Qiu
  - [![GitHub stars](https://img.shields.io/github/stars/main/speechgpt?style=social)](https://github.com/0nutation/SpeechGPT/tree/main/speechgpt)
  - [Paper](https://arxiv.org/pdf/2305.11000.pdf) / [Demo](https://0nutation.github.io/SpeechGPT.github.io/)

- `ã€2023-04ã€‘-ã€AudioGPTã€‘-ã€Zhejiang Universityã€‘-ã€Type: Modelã€‘`
  - **AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head**
  - **Author(s):** Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, Yi Ren, Zhou Zhao, Shinji Watanabe
  - [![GitHub stars](https://img.shields.io/github/stars/AIGC-Audio/AudioGPT?style=social)](https://github.com/AIGC-Audio/AudioGPT)
  - [Paper](https://arxiv.org/pdf/2304.12995.pdf)

## Benchmark

- `ã€2024-12ã€‘-ã€TalkArenaã€‘-ã€Stanford University, SCB 10Xã€‘-ã€Type: Interactive Benchmarking Toolã€‘`
  - **TalkArena: Interactive Evaluation of Large Audio Models**
  - **Author(s):** Ella Minzhi Li*, Will Held*, Michael J. Ryan, Kunat Pipatanakul, Potsawee Manakul, Hao Zhu, Diyi Yang (*Equal Contribution)
  - [![GitHub stars](https://img.shields.io/github/stars/SALT-NLP/talk-arena?style=social)](https://github.com/SALT-NLP/talk-arena)
  - [Demo](https://talkarena.org/) / [Other Link](https://talkarena.org/blog)

- `ã€2024-12ã€‘-ã€ADU-Benchã€‘-ã€Tsinghua University, University of Oxfordã€‘-ã€Type: Benchmarkã€‘`
  - **Benchmarking Open-ended Audio Dialogue Understanding for Large Audio-Language Models**
  - **Author(s):** Kuofeng Gao, Shu-Tao Xia, Ke Xu, Philip Torr, Jindong Gu
  - [Paper](https://arxiv.org/abs/2412.05167)

- `ã€2024-11ã€‘-ã€Dynamic-SUPERB Phase-2ã€‘-ã€National Taiwan University, University of Texas at Austin, Carnegie Mellon University, Nanyang Technological University, Toyota Technological Institute of Chicago, UniversitÃ© du QuÃ©bec (INRS-EMT), NVIDIA, ASAPP, Renmin University of Chinaã€‘-ã€Type: Evaluation Frameworkã€‘`
  - **Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks**
  - **Author(s):** Chien-yu Huang, Wei-Chih Chen, Shu-wen Yang, Andy T. Liu, Chen-An Li, Yu-Xiang Lin, Wei-Cheng Tseng, Anuj Diwan, Yi-Jen Shih, Jiatong Shi, William Chen, Xuanjun Chen, Chi-Yuan Hsiao, Puyuan Peng, Shih-Heng Wang, Chun-Yi Kuan, Haibin Wu, Siddhant Arora, Kai-Wei Chang, Yifan Peng, Roshan Sharma, Shinji Watanabe, Bhiksha Ramakrishnan, Shady Shehata, Hung-yi Lee
  - [![GitHub stars](https://img.shields.io/github/stars/dynamic-superb/dynamic-superb?style=social)](https://github.com/dynamic-superb/dynamic-superb)
  - [Paper](https://arxiv.org/pdf/2411.05361) / [Other Link](https://dynamic-superb.github.io/)

- `ã€2024-10ã€‘-ã€VoiceBenchã€‘-ã€National University of Singaporeã€‘-ã€Type: Benchmarkã€‘`
  - **VoiceBench: Benchmarking LLM-Based Voice Assistants**
  - **Author(s):** Yiming Chen, Xianghu Yue, Chen Zhang, Xiaoxue Gao, Robby T. Tan, Haizhou Li
  - [![GitHub stars](https://img.shields.io/github/stars/MatthewCYM/VoiceBench?style=social)](https://github.com/MatthewCYM/VoiceBench)
  - [Paper](https://arxiv.org/pdf/2410.17196)

- `ã€2024-10ã€‘-ã€MMAUã€‘-ã€University of Marylandã€‘-ã€Type: Benchmarkã€‘`
  - **MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark**
  - **Author(s):** S Sakshi, Utkarsh Tyagi, Sonal Kumar, Ashish Seth, Ramaneswaran Selvakumar, Oriol Nieto, Ramani Duraiswami, Sreyan Ghosh, Dinesh Manocha
  - [![GitHub stars](https://img.shields.io/github/stars/tree/main?style=social)](https://github.com/Sakshi113/mmau/tree/main)
  - [Paper](https://arxiv.org/pdf/2410.19168) / [Other Link](https://sakshi113.github.io/mmau_homepage/)

- `ã€2024-09ã€‘-ã€SALMonã€‘-ã€Hebrew University of Jerusalemã€‘-ã€Type: Benchmarkã€‘`
  - **A Suite for Acoustic Language Model Evaluation**
  - **Author(s):** Gallil Maimon, Amit Roth, Yossi Adi
  - [![GitHub stars](https://img.shields.io/github/stars/adiyoss-lab/salmon?style=social)](https://github.com/adiyoss-lab/salmon)
  - [Paper](https://arxiv.org/abs/2409.07437) / [Demo](https://pages.cs.huji.ac.il/adiyoss-lab/salmon/)

- `ã€2024-08ã€‘-ã€MuChoMusicã€‘-ã€UPF, QMUL, UMGã€‘-ã€Type: Benchmarkã€‘`
  - **MuChoMusic: Evaluating Music Understanding in Multimodal Audio-Language Models**
  - **Author(s):** Benno Weck, Ilaria Manco, Emmanouil Benetos, Elio Quinton, George Fazekas, Dmitry Bogdanov
  - [![GitHub stars](https://img.shields.io/github/stars/mulab-mir/muchomusic?style=social)](https://github.com/mulab-mir/muchomusic)
  - [Paper](https://arxiv.org/abs/2408.01337)

- `ã€2024-07ã€‘-ã€AudioEntailmentã€‘-ã€CMU, Microsoftã€‘-ã€Type: Benchmarkã€‘`
  - **Audio Entailment: Assessing Deductive Reasoning for Audio Understanding**
  - **Author(s):** Soham Deshmukh, Shuo Han, Hazim Bukhari, Benjamin Elizalde, Hannes Gamper, Rita Singh, Bhiksha Raj
  - [![GitHub stars](https://img.shields.io/github/stars/microsoft/AudioEntailment?style=social)](https://github.com/microsoft/AudioEntailment)
  - [Paper](https://arxiv.org/pdf/2407.18062)

- `ã€2024-06ã€‘-ã€SD-Evalã€‘-ã€CUHK, Bytedanceã€‘-ã€Type: Benchmarkã€‘`
  - **SD-Eval: A Benchmark Dataset for Spoken Dialogue Understanding Beyond Words**
  - **Author(s):** Junyi Ao, Yuancheng Wang, Xiaohai Tian, Dekun Chen, Jun Zhang, Lu Lu, Yuxuan Wang, Haizhou Li, Zhizheng Wu
  - [![GitHub stars](https://img.shields.io/github/stars/amphionspace/SD-Eval?style=social)](https://github.com/amphionspace/SD-Eval)
  - [Paper](https://arxiv.org/pdf/2406.13340)

- `ã€2024-06ã€‘-ã€AudioBenchã€‘-ã€A*STAR, Singaporeã€‘-ã€Type: Benchmarkã€‘`
  - **AudioBench: A Universal Benchmark for Audio Large Language Models**
  - **Author(s):** Bin Wang, Xunlong Zou, Geyu Lin, Shuo Sun, Zhuohan Liu, Wenyu Zhang, Zhengyuan Liu, AiTi Aw, Nancy F. Chen
  - [![GitHub stars](https://img.shields.io/github/stars/AudioLLMs/AudioBench?style=social)](https://github.com/AudioLLMs/AudioBench)
  - [Paper](https://arxiv.org/abs/2406.16020) / [Demo](https://huggingface.co/spaces/AudioLLMs/AudioBench-Leaderboard)

- `ã€2024-05ã€‘-ã€AIR-Benchã€‘-ã€ZJU, Alibabaã€‘-ã€Type: Benchmarkã€‘`
  - **AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension**
  - **Author(s):** Qian Yang, Jin Xu, Wenrui Liu, Yunfei Chu, Ziyue Jiang, Xiaohuan Zhou, Yichong Leng, Yuanjun Lv, Zhou Zhao, Chang Zhou, Jingren Zhou
  - [![GitHub stars](https://img.shields.io/github/stars/OFA-Sys/AIR-Bench?style=social)](https://github.com/OFA-Sys/AIR-Bench)
  - [Paper](https://aclanthology.org/2024.acl-long.109/)

- `ã€2024-03ã€‘-ã€SpokenWOZã€‘-ã€Tencentã€‘-ã€Type: Benchmarkã€‘`
  - **SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented Dialogue Agents**
  - **Author(s):** Shuzheng Si, Wentao Ma, Haoyu Gao, Yuchuan Wu, Ting-En Lin, Yinpei Dai, Hangyu Li, Rui Yan, Fei Huang, Yongbin Li
  - [![GitHub stars](https://img.shields.io/github/stars/main/spokenwoz?style=social)](https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/spokenwoz)
  - [Paper](https://arxiv.org/abs/2305.13040) / [Demo](https://spokenwoz.github.io/)

- `ã€2023-09ã€‘-ã€Dynamic-SUPERBã€‘-ã€NTU-Taiwan, etc.ã€‘-ã€Type: Benchmarkã€‘`
  - **Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech**
  - **Author(s):** Chien-yu Huang, Ke-Han Lu, Shih-Heng Wang, Chi-Yuan Hsiao, Chun-Yi Kuan, Haibin Wu, Siddhant Arora, Kai-Wei Chang, Jiatong Shi, Yifan Peng, Roshan Sharma, Shinji Watanabe, Bhiksha Ramakrishnan, Shady Shehata, Hung-yi Lee
  - [![GitHub stars](https://img.shields.io/github/stars/dynamic-superb/dynamic-superb?style=social)](https://github.com/dynamic-superb/dynamic-superb)
  - [Paper](https://arxiv.org/abs/2309.09510)

## Multimodal

- `ã€2024-09ã€‘-ã€EMOVAã€‘-ã€HKUSTã€‘-ã€Type: Modelã€‘`
  - **EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions**
  - **Author(s):** Kai Chen, Yunhao Gou, Runhui Huang, Zhili Liu, Daxin Tan, Jing Xu, Chunwei Wang, Yi Zhu, Yihan Zeng, Kuo Yang, Dingdong Wang, Kun Xiang, Haoyuan Li, Haoli Bai, Jianhua Han, Xiaohui Li, Weike Jin, Nian Xie, Yu Zhang, James T. Kwok, Hengshuang Zhao, Xiaodan Liang, Dit-Yan Yeung, Xiao Chen, Zhenguo Li, Wei Zhang, Qun Liu, Jun Yao, Lanqing Hong, Lu Hou, Hang Xu
  - [Paper](https://arxiv.org/pdf/2409.18042) / [Demo](https://emova-ollm.github.io/)

- `ã€2023-11ã€‘-ã€CoDi-2ã€‘-ã€UC Berkeleyã€‘-ã€Type: Modelã€‘`
  - **CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation**
  - **Author(s):** Zineng Tang, Ziyi Yang, Mahmoud Khademi, Yang Liu, Chenguang Zhu, Mohit Bansal
  - [![GitHub stars](https://img.shields.io/github/stars/main/CoDi-2?style=social)](https://github.com/microsoft/i-Code/tree/main/CoDi-2)
  - [Paper](https://arxiv.org/pdf/2311.18775) / [Demo](https://codi-2.github.io/)

- `ã€2023-06ã€‘-ã€Macaw-LLMã€‘-ã€Tencentã€‘-ã€Type: Modelã€‘`
  - **Macaw-LLM: Multi-Modal Language Modeling with Image, Video, Audio, and Text Integration**
  - **Author(s):** Chenyang Lyu, Minghao Wu, Longyue Wang, Xinting Huang, Bingshuai Liu, Zefeng Du, Shuming Shi, Zhaopeng Tu
  - [![GitHub stars](https://img.shields.io/github/stars/lyuchenyang/Macaw-LLM?style=social)](https://github.com/lyuchenyang/Macaw-LLM)
  - [Paper](https://arxiv.org/pdf/2306.09093)

## Survey

- `ã€2024-11ã€‘-ã€WavChat-Surveyã€‘-ã€Zhejiang Universityã€‘-ã€Type: Surveyã€‘`
  - **WavChat: A Survey of Spoken Dialogue Models**
  - **Author(s):** Shengpeng Ji, Yifu Chen, Minghui Fang, Jialong Zuo, Jingyu Lu, Hanting Wang, Ziyue Jiang, Long Zhou, Shujie Liu, Xize Cheng, Xiaoda Yang, Zehan Wang, Qian Yang, Jian Li, Yidi Jiang, Jingzhen He, Yunfei Chu, Jin Xu, Zhou Zhao
  - [Paper](https://arxiv.org/abs/2411.13577)

- `ã€2024-10ã€‘-ã€SpeechLLM-Surveyã€‘-ã€SJTU, AISpeechã€‘-ã€Type: Surveyã€‘`
  - **A Survey on Speech Large Language Models**
  - **Author(s):** Jing Peng, Yucheng Wang, Yu Xi, Xu Li, Xizhuo Zhang, Kai Yu
  - [Paper](https://arxiv.org/pdf/2410.18908v2)

- `ã€2024-10ã€‘-ã€SpeechLM-Surveyã€‘-ã€CUHK, Tencentã€‘-ã€Type: Surveyã€‘`
  - **Recent Advances in Speech Language Models: A Survey**
  - **Author(s):** Wenqian Cui, Dianzhi Yu, Xiaoqi Jiao, Ziqiao Meng, Guangyan Zhang, Qichao Wang, Yiwen Guo, Irwin King
  - [Paper](https://arxiv.org/pdf/2410.03751)

- `ã€2024-02ã€‘-ã€AudioLM-Surveyã€‘-ã€National Taiwan University, MITã€‘-ã€Type: Surveyã€‘`
  - **Towards audio language modeling -- an overview**
  - **Author(s):** Haibin Wu, Xuanjun Chen, Yi-Cheng Lin, Kai-wei Chang, Ho-Lam Chung, Alexander H. Liu, Hung-yi Lee
  - [Paper](https://arxiv.org/abs/2402.13236)

## Study

- `ã€2024-06ã€‘-ã€Audio Hallucinationã€‘-ã€NTU-Taiwanã€‘-ã€Type: Researchã€‘`
  - **Understanding Sounds, Missing the Questions: The Challenge of Object Hallucination in Large Audio-Language Models**
  - **Author(s):** Chun-Yi Kuan, Wei-Ping Huang, Hung-yi Lee
  - [![GitHub stars](https://img.shields.io/github/stars/kuan2jiu99/audio-hallucination?style=social)](https://github.com/kuan2jiu99/audio-hallucination)
  - [Paper](https://arxiv.org/pdf/2406.08402)

## Safety

- `ã€2024-06ã€‘-ã€CodecFakeã€‘-ã€National Taiwan Universityã€‘-ã€Type: Safetyã€‘`
  - **CodecFake: Enhancing Anti-Spoofing Models Against Deepfake Audios from Codec-Based Speech Synthesis Systems**
  - **Author(s):** Haibin Wu, Yuan Tseng, Hung-yi Lee
  - [![GitHub stars](https://img.shields.io/github/stars/roger-tseng/CodecFake?style=social)](https://github.com/roger-tseng/CodecFake)
  - [Paper](https://arxiv.org/abs/2406.07237) / [Other Link](https://codecfake.github.io/)

- `ã€2024-05ã€‘-ã€VoiceJailbreakã€‘-ã€CISPAã€‘-ã€Type: Methodã€‘`
  - **Voice Jailbreak Attacks Against GPT-4o**
  - **Author(s):** Xinyue Shen, Yixin Wu, Michael Backes, Yang Zhang
  - [![GitHub stars](https://img.shields.io/github/stars/TrustAIRLab/VoiceJailbreakAttack?style=social)](https://github.com/TrustAIRLab/VoiceJailbreakAttack)
  - [Paper](https://arxiv.org/pdf/2405.19103)

## Chatbot

- `ã€2025-01ã€‘-ã€MinMoã€‘-ã€FunAudioLLM Team, Tongyi Lab, Alibaba Groupã€‘-ã€Type: Multimodal Large Language Modelã€‘`
  - **MinMo: A Multimodal Large Language Model for Seamless Voice Interaction**
  - **Author(s):** Qian Chen, Yafeng Chen, Yanni Chen, Mengzhe Chen, Yingda Chen, Chong Deng, Zhihao Du, Ruize Gao, Changfeng Gao, Zhifu Gao, Yabin Li, Xiang Lv, Jiaqing Liu, Haoneng Luo, Bin Ma, Chongjia Ni, Xian Shi, Jialong Tang, Hui Wang, Hao Wang, Wen Wang, Yuxuan Wang, Yunlan Xu, Fan Yu, Zhijie Yan, Yexin Yang, Baosong Yang, Xian Yang, Guanrou Yang, Tianyu Zhao, Qinglin Zhang, Shiliang Zhang, Nan Zhao, Pei Zhang, Chong Zhang, Jinren Zhou
  - [Paper](https://arxiv.org/abs/2501.06282) / [Other Link](https://funaudiollm.github.io/minmo)
